{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nA22T8kQEz7_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n"
      ],
      "metadata": {
        "id": "5i9NzNwhE5mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "\n",
        "def display_core_components(config_file_path):\n",
        "    # Create a ConfigParser object\n",
        "    config = configparser.ConfigParser()\n",
        "\n",
        "    # Read the configuration file\n",
        "    config.read(config_file_path)\n",
        "\n",
        "    # Get the core components from the configuration file\n",
        "    core_components = config.get('core-site', 'fs.defaultFS')\n",
        "\n",
        "    # Display the core components\n",
        "    print(\"Core Components of Hadoop:\")\n",
        "    print(core_components)\n",
        "\n",
        "# Path to the Hadoop configuration file (core-site.xml)\n",
        "config_file_path = \"/path/to/core-site.xml\"\n",
        "\n",
        "# Call the function to display the core components\n",
        "display_core_components(config_file_path)\n"
      ],
      "metadata": {
        "id": "b_3D1cM0E6TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n"
      ],
      "metadata": {
        "id": "u5nlhEbvE_cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
        "\n",
        "def calculate_directory_size(hdfs_host, hdfs_port, hdfs_user, hdfs_directory):\n",
        "    # Create a PyWebHdfsClient object\n",
        "    client = PyWebHdfsClient(host=hdfs_host, port=hdfs_port, user_name=hdfs_user)\n",
        "\n",
        "    # Get the file status of the HDFS directory\n",
        "    directory_status = client.list_dir(hdfs_directory)\n",
        "\n",
        "    # Initialize the total file size\n",
        "    total_size = 0\n",
        "\n",
        "    # Iterate through each file in the directory\n",
        "    for file_info in directory_status['FileStatuses']['FileStatus']:\n",
        "        # Get the file size\n",
        "        file_size = int(file_info['length'])\n",
        "\n",
        "        # Add the file size to the total\n",
        "        total_size += file_size\n",
        "\n",
        "    return total_size\n",
        "\n",
        "# HDFS connection details\n",
        "hdfs_host = \"your_hdfs_host\"\n",
        "hdfs_port = 50070  # Default port for HDFS NameNode\n",
        "hdfs_user = \"your_hdfs_username\"\n",
        "\n",
        "# HDFS directory for which to calculate the total size\n",
        "hdfs_directory = \"/path/to/hdfs_directory\"\n",
        "\n",
        "# Call the function to calculate the total file size\n",
        "total_file_size = calculate_directory_size(hdfs_host, hdfs_port, hdfs_user, hdfs_directory)\n",
        "\n",
        "# Display the total file size\n",
        "print(\"Total File Size: {} bytes\".format(total_file_size))\n"
      ],
      "metadata": {
        "id": "cMiipUX4FCK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
      ],
      "metadata": {
        "id": "upefrgEvGIPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "\n",
        "def display_core_components(config_file_path):\n",
        "    # Create a ConfigParser object\n",
        "    config = configparser.ConfigParser()\n",
        "\n",
        "    # Read the configuration file\n",
        "    config.read(config_file_path)\n",
        "\n",
        "    # Get the core components from the configuration file\n",
        "    core_components = config.get('core-site', 'fs.defaultFS')\n",
        "\n",
        "    # Display the core components\n",
        "    print(\"Core Components of Hadoop:\")\n",
        "    print(core_components)\n",
        "\n",
        "# Path to the Hadoop configuration file (core-site.xml)\n",
        "config_file_path = \"/path/to/core-site.xml\"\n",
        "\n",
        "# Call the function to display the core components\n",
        "display_core_components(config_file_path)\n"
      ],
      "metadata": {
        "id": "NiIkfZD_GmCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
      ],
      "metadata": {
        "id": "Lf62KpycGL4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def check_hadoop_cluster_health(namenode_host, namenode_port):\n",
        "    # API endpoint URLs\n",
        "    namenode_health_url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
        "    datanode_health_url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
        "\n",
        "    # Check NameNode health\n",
        "    namenode_health = check_health_status(namenode_health_url)\n",
        "    print(\"NameNode Health Status:\")\n",
        "    print(namenode_health)\n",
        "\n",
        "    # Check DataNode health\n",
        "    datanode_health = check_health_status(datanode_health_url)\n",
        "    print(\"\\nDataNode Health Status:\")\n",
        "    print(datanode_health)\n",
        "\n",
        "def check_health_status(url):\n",
        "    try:\n",
        "        # Send GET request to Hadoop REST API\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse JSON response\n",
        "        response_data = response.json()\n",
        "\n",
        "        # Extract health status from response\n",
        "        health_status = response_data['beans'][0]['State']\n",
        "\n",
        "        return health_status\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(\"Error occurred while checking health status:\", str(e))\n",
        "        return None\n",
        "\n",
        "# Hadoop cluster details\n",
        "namenode_host = \"your_namenode_host\"\n",
        "namenode_port = 50070  # Default port for Hadoop NameNode REST API\n",
        "\n",
        "# Call the function to check Hadoop cluster health\n",
        "check_hadoop_cluster_health(namenode_host, namenode_port)\n"
      ],
      "metadata": {
        "id": "hSVxHGwhGyy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Develop a Python program that lists all the files and directories in a specific HDFS path."
      ],
      "metadata": {
        "id": "eewoXxZ1G-Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
        "\n",
        "def list_hdfs_path(hdfs_host, hdfs_port, hdfs_user, hdfs_path):\n",
        "    # Create a PyWebHdfsClient object\n",
        "    client = PyWebHdfsClient(host=hdfs_host, port=hdfs_port, user_name=hdfs_user)\n",
        "\n",
        "    # List the files and directories in the HDFS path\n",
        "    directory_contents = client.list_dir(hdfs_path)\n",
        "\n",
        "    # Iterate through each item in the directory contents\n",
        "    for item in directory_contents['FileStatuses']['FileStatus']:\n",
        "        # Check if it is a file or a directory\n",
        "        if item['type'] == 'DIRECTORY':\n",
        "            print(\"[Directory] {}\".format(item['pathSuffix']))\n",
        "        else:\n",
        "            print(\"[File]      {}\".format(item['pathSuffix']))\n",
        "\n",
        "# HDFS connection details\n",
        "hdfs_host = \"your_hdfs_host\"\n",
        "hdfs_port = 50070  # Default port for HDFS NameNode\n",
        "hdfs_user = \"your_hdfs_username\"\n",
        "\n",
        "# HDFS path to list\n",
        "hdfs_path = \"/path/to/hdfs_directory\"\n",
        "\n",
        "# Call the function to list the files and directories\n",
        "list_hdfs_path(hdfs_host, hdfs_port, hdfs_user, hdfs_path)\n"
      ],
      "metadata": {
        "id": "HU6SCu2vHBHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
      ],
      "metadata": {
        "id": "09KRjW-iHNhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def analyze_data_node_storage(hdfs_host, hdfs_port):\n",
        "    # API endpoint URL for getting DataNode information\n",
        "    datanode_info_url = f\"http://{hdfs_host}:{hdfs_port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
        "\n",
        "    # Send GET request to Hadoop REST API\n",
        "    response = requests.get(datanode_info_url)\n",
        "\n",
        "    # Parse JSON response\n",
        "    response_data = response.json()\n",
        "\n",
        "    # Extract DataNode information from response\n",
        "    datanode_info = response_data['beans'][0]\n",
        "\n",
        "    # Get storage information of each DataNode\n",
        "    storage_info = json.loads(datanode_info['StorageInfo'])\n",
        "\n",
        "    # Extract storage capacity and utilization of each DataNode\n",
        "    data_nodes = storage_info['DataNodeStorageInfo']\n",
        "    data_nodes_info = []\n",
        "    for data_node in data_nodes:\n",
        "        node_name = data_node['datanode']\n",
        "        capacity = data_node['capacity']\n",
        "        remaining = data_node['remaining']\n",
        "        utilization = 100 - (float(remaining) / float(capacity)) * 100\n",
        "\n",
        "        data_nodes_info.append({\n",
        "            'node_name': node_name,\n",
        "            'capacity': capacity,\n",
        "            'remaining': remaining,\n",
        "            'utilization': utilization\n",
        "        })\n",
        "\n",
        "    # Sort DataNodes based on storage capacity\n",
        "    sorted_data_nodes = sorted(data_nodes_info, key=lambda x: x['capacity'])\n",
        "\n",
        "    # Print DataNodes with highest and lowest storage capacities\n",
        "    print(\"DataNode with highest storage capacity:\")\n",
        "    print(sorted_data_nodes[-1])\n",
        "\n",
        "    print(\"\\nDataNode with lowest storage capacity:\")\n",
        "    print(sorted_data_nodes[0])\n",
        "\n",
        "# HDFS cluster details\n",
        "hdfs_host = \"your_hdfs_host\"\n",
        "hdfs_port = 50070  # Default port for Hadoop NameNode REST API\n",
        "\n",
        "# Call the function to analyze DataNode storage utilization\n",
        "analyze_data_node_storage(hdfs_host, hdfs_port)\n"
      ],
      "metadata": {
        "id": "W8QC2_ojHRXL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}